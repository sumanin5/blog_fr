# GitOps æ¶æ„è®¾è®¡æ–‡æ¡£

## ğŸ“ ç³»ç»Ÿæ¶æ„

### æ•´ä½“æ¶æ„å›¾

```mermaid
graph TB
    subgraph "è§¦å‘å±‚"
        A1[ç®¡ç†å‘˜æ‰‹åŠ¨è§¦å‘]
        A2[å®šæ—¶ä»»åŠ¡ - æœªå®ç°]
        A3[Git Webhook - æœªå®ç°]
    end

    subgraph "API å±‚"
        B[GitOps Router<br>/ops/git/sync]
    end

    subgraph "ä¸šåŠ¡é€»è¾‘å±‚"
        C[GitOpsService<br>åŒæ­¥ç¼–æ’]
        D[MDXScanner<br>æ–‡ä»¶æ‰«æ]
        E[FrontmatterMapper<br>å­—æ®µæ˜ å°„]
        F[AuthorResolver<br>ä½œè€…è§£æ]
        G[CoverResolver<br>å°é¢è§£æ]
        H[GitClient<br>Gitæ“ä½œ]
    end

    subgraph "æ•°æ®å±‚"
        I[PostService<br>æ–‡ç«  CRUD]
        J[(PostgreSQL)]
        K[æ–‡ä»¶ç³»ç»Ÿ<br>content/]
    end

    A1 -->|HTTP POST| B
    A2 -.->|APScheduler| C
    A3 -.->|Webhook| C

    B -->|è®¤è¯æˆæƒ| B
    B -->|è°ƒç”¨| C

    C -->|æ‰«ææ–‡ä»¶| D
    C -->|æ˜ å°„å­—æ®µ| E
    C -->|Git Pull| H

    E -->|è§£æä½œè€…| F
    E -->|è§£æå°é¢| G

    D -->|è¯»å–| K
    H -->|å‘½ä»¤è¡Œ| K

    C -->|CRUD| I
    I -->|SQL| J
    F -->|æŸ¥è¯¢| J
    G -->|æŸ¥è¯¢| J

    style A2 stroke-dasharray: 5 5
    style A3 stroke-dasharray: 5 5
```

---

## ğŸ”„ æ ¸å¿ƒæµç¨‹è®¾è®¡

### 1. å®Œæ•´åŒæ­¥æµç¨‹

```mermaid
flowchart TD
    Start([å¼€å§‹åŒæ­¥]) --> Init[åˆå§‹åŒ– GitOpsService]
    Init --> ValidateConfig{éªŒè¯ CONTENT_DIR}

    ValidateConfig -->|å¤±è´¥| ErrorConfig[æŠ›å‡º GitOpsConfigurationError]
    ValidateConfig -->|æˆåŠŸ| GetUser{è·å–æ“ä½œç”¨æˆ·}

    GetUser -->|å·²æä¾›| UseProvided[ä½¿ç”¨ default_user]
    GetUser -->|æœªæä¾›| QuerySuperAdmin[æŸ¥è¯¢ Superadmin]

    UseProvided --> GitPull[å°è¯• Git Pull]
    QuerySuperAdmin -->|æ‰¾åˆ°| GitPull
    QuerySuperAdmin -->|æœªæ‰¾åˆ°| ErrorNoUser[æŠ›å‡ºé…ç½®é”™è¯¯]

    GitPull -->|æˆåŠŸ| StartScan[å¼€å§‹æ‰«æ]
    GitPull -->|å¤±è´¥| LogWarning[è®°å½•è­¦å‘Š]
    LogWarning --> StartScan

    StartScan --> ScanFiles[MDXScanner.scan_all]
    ScanFiles --> BuildScannedMap[æ„å»º scanned_map]

    BuildScannedMap --> QueryDB[æŸ¥è¯¢æ•°æ®åº“]
    QueryDB --> FilterSourcePath[è¿‡æ»¤ source_path IS NOT NULL]
    FilterSourcePath --> BuildExistingMap[æ„å»º existing_map]

    BuildExistingMap --> ProcessFiles[å¤„ç†æ–‡ä»¶]
    ProcessFiles --> FileLoop{éå† scanned_map}

    FileLoop -->|æ¯ä¸ªæ–‡ä»¶| CheckExists{existing_map ä¸­å­˜åœ¨?}

    CheckExists -->|æ˜¯| MapToUpdate[è°ƒç”¨ mapper.map_to_post]
    CheckExists -->|å¦| MapToCreate[è°ƒç”¨ mapper.map_to_post]

    MapToUpdate --> ResolveAuthor1[è§£æä½œè€…]
    MapToCreate --> ResolveAuthor2[è§£æä½œè€…]

    ResolveAuthor1 --> ResolveCover1[è§£æå°é¢]
    ResolveAuthor2 --> ResolveCover2[è§£æå°é¢]

    ResolveCover1 --> ValidateUpdate{Pydantic éªŒè¯}
    ResolveCover2 --> ValidateCreate{Pydantic éªŒè¯}

    ValidateUpdate -->|æˆåŠŸ| CallUpdate[post_service.update_post]
    ValidateUpdate -->|å¤±è´¥| LogUpdateError[è®°å½•åˆ° errors]

    ValidateCreate -->|æˆåŠŸ| CallCreate[post_service.create_post]
    ValidateCreate -->|å¤±è´¥| LogCreateError[è®°å½•åˆ° errors]

    CallUpdate --> RecordUpdated[stats.updated.append]
    CallCreate --> RecordAdded[stats.added.append]
    LogUpdateError --> FileLoop
    LogCreateError --> FileLoop
    RecordUpdated --> FileLoop
    RecordAdded --> FileLoop

    FileLoop -->|å®Œæˆ| ProcessDeleted[å¤„ç†åˆ é™¤]
    ProcessDeleted --> DBLoop{éå† existing_map}

    DBLoop -->|æ¯ä¸ªæ–‡ç« | CheckFileExists{scanned_map ä¸­å­˜åœ¨?}

    CheckFileExists -->|å¦| CallDelete[post_service.delete_post]
    CheckFileExists -->|æ˜¯| DBLoop

    CallDelete --> RecordDeleted[stats.deleted.append]
    RecordDeleted --> DBLoop

    DBLoop -->|å®Œæˆ| CalcDuration[è®¡ç®—æ€»è€—æ—¶]
    CalcDuration --> ReturnStats[è¿”å› SyncStats]

    ReturnStats --> End([ç»“æŸ])
    ErrorConfig --> End
    ErrorNoUser --> End
```

### 2. æ–‡ä»¶æ‰«ææµç¨‹

```mermaid
flowchart LR
    A[å¼€å§‹æ‰«æ] --> B[glob **/*.md<br>**/*.mdx]
    B --> C{éå†åŒ¹é…æ–‡ä»¶}

    C --> D[è®¡ç®—ç›¸å¯¹è·¯å¾„]
    D --> E[asyncio.to_thread<br>è¯»å–æ–‡ä»¶]
    E --> F[frontmatter.loads<br>è§£æ]
    F --> G[æå– metadata]
    F --> H[æå– content]

    G --> I[JSON åºåˆ—åŒ–]
    I --> J[SHA256 meta_hash]

    E --> K[åŸå§‹å†…å®¹]
    K --> L[SHA256 content_hash]

    J --> M[æ„å»º ScannedPost]
    L --> M
    H --> M
    D --> M

    M --> N[è·å–æ–‡ä»¶ mtime]
    N --> O[åŠ å…¥ç»“æœåˆ—è¡¨]

    O --> C
    C -->|ç»“æŸ| P([è¿”å› List])
```

### 3. Frontmatter æ˜ å°„æµç¨‹

```mermaid
flowchart TD
    A[ScannedPost] --> B[è¯»å– frontmatter]

    B --> C[è§£æåŸºç¡€å­—æ®µ]
    C --> C1[title]
    C --> C2[slug]
    C --> C3[excerpt]
    C --> C4[content_mdx]

    B --> D[è§£æçŠ¶æ€å­—æ®µ]
    D --> D1[status]
    D --> D2[published_at]

    B --> E[è§£æå¸ƒå°”å­—æ®µ]
    E --> E1[is_featured]
    E --> E2[allow_comments]

    B --> F[è§£æ SEO å­—æ®µ]
    F --> F1[meta_title]
    F --> F2[meta_description]
    F --> F3[meta_keywords]

    B --> G[è§£æå¼•ç”¨å­—æ®µ]
    G --> G1[author]
    G --> G2[cover]
    G --> G3[tags]

    G1 --> H1[AuthorResolver.resolve]
    H1 --> H1A{æŸ¥è¯¢ç”¨æˆ·}
    H1A -->|æ‰¾åˆ°| H1B[è¿”å› author_id]
    H1A -->|æœªæ‰¾åˆ°| H1C[æŠ›å‡º GitOpsSyncError]

    G2 --> H2[CoverResolver.resolve]
    H2 --> H2A{æŸ¥è¯¢åª’ä½“æ–‡ä»¶}
    H2A -->|æ‰¾åˆ°| H2B[è¿”å› cover_media_id]
    H2A -->|æœªæ‰¾åˆ°| H2C[è¿”å› None]

    G3 --> H3[è§£ææ ‡ç­¾]
    H3 --> H3A{æ˜¯å¦ä¸ºå­—ç¬¦ä¸²?}
    H3A -->|æ˜¯| H3B[é€—å·åˆ†éš”]
    H3A -->|å¦| H3C[ç›´æ¥ä½¿ç”¨æ•°ç»„]

    C1 --> I[åˆå¹¶æ‰€æœ‰å­—æ®µ]
    C2 --> I
    C3 --> I
    C4 --> I
    D1 --> I
    D2 --> I
    E1 --> I
    E2 --> I
    F1 --> I
    F2 --> I
    F3 --> I
    H1B --> I
    H2B --> I
    H3B --> I
    H3C --> I

    I --> J[è¿”å› post_dict]
```

---

## ğŸ—‚ï¸ æ¨¡å—èŒè´£åˆ’åˆ†

### 1. `router.py` - API å…¥å£å±‚

**èŒè´£**:

- å®šä¹‰ HTTP ç«¯ç‚¹
- æƒé™è®¤è¯ï¼ˆéœ€è¦ç®¡ç†å‘˜ï¼‰
- ä¾èµ–æ³¨å…¥ï¼ˆSessionã€Userï¼‰
- è°ƒç”¨ Service å±‚

**å…³é”®ä»£ç **:

```python
@router.post("/sync", response_model=SyncStats)
async def trigger_sync(
    current_user: User = Depends(get_current_adminuser),
    session: AsyncSession = Depends(get_async_session),
):
    service = GitOpsService(session)
    return await service.sync_all(default_user=current_user)
```

**è®¾è®¡åŸåˆ™**:

- **è–„å±‚è®¾è®¡**: Router åªè´Ÿè´£ HTTP å±‚é¢çš„äº‹æƒ…
- **ä¾èµ–æ³¨å…¥**: ä½¿ç”¨ FastAPI çš„ Depends æœºåˆ¶
- **æƒé™æ§åˆ¶**: é€šè¿‡ `get_current_adminuser` ç¡®ä¿åªæœ‰ç®¡ç†å‘˜å¯è®¿é—®

---

### 2. `service.py` - ä¸šåŠ¡é€»è¾‘å±‚

**èŒè´£**:

- åŒæ­¥æµç¨‹ç¼–æ’
- å¢åˆ æ”¹æŸ¥å†³ç­–
- é”™è¯¯å¤„ç†ä¸ç»Ÿè®¡
- è°ƒç”¨ Scannerã€Mapper å’Œ PostService

**æ ¸å¿ƒæ–¹æ³•**:

| æ–¹æ³•                  | åŠŸèƒ½           | å¤æ‚åº¦ |
| --------------------- | -------------- | ------ |
| `sync_all()`          | ä¸»åŒæ­¥æµç¨‹     | O(n)   |
| `_sync_single_file()` | å•æ–‡ä»¶åŒæ­¥é€»è¾‘ | O(1)   |

**æ•°æ®ç»“æ„**:

```python
class SyncStats(BaseModel):
    added: List[str]      # æ–°å¢æ–‡ä»¶è·¯å¾„
    updated: List[str]    # æ›´æ–°æ–‡ä»¶è·¯å¾„
    deleted: List[str]    # åˆ é™¤æ–‡ä»¶è·¯å¾„
    skipped: int          # è·³è¿‡æ•°é‡
    errors: List[str]     # é”™è¯¯ä¿¡æ¯
    duration: float       # æ€»è€—æ—¶ï¼ˆç§’ï¼‰
```

**è®¾è®¡æ¨¡å¼**:

- **ç¼–æ’è€…æ¨¡å¼**: Service ä½œä¸ºç¼–æ’è€…ï¼Œåè°ƒå„ä¸ªç»„ä»¶
- **é”™è¯¯éš”ç¦»**: å•æ–‡ä»¶å¤±è´¥ä¸å½±å“æ•´ä½“æµç¨‹
- **ç»Ÿè®¡æ”¶é›†**: ä½¿ç”¨ `SyncStats` æ”¶é›†æ‰€æœ‰æ“ä½œç»“æœ

---

### 3. `scanner.py` - æ–‡ä»¶æ‰«æå±‚

**èŒè´£**:

- æ–‡ä»¶ç³»ç»Ÿéå†
- Frontmatter è§£æ
- å“ˆå¸Œè®¡ç®—
- å¼‚æ­¥ I/O å¤„ç†

**æ ¸å¿ƒç±»**:

```python
class ScannedPost(BaseModel):
    file_path: str         # ç›¸å¯¹è·¯å¾„
    content_hash: str      # å…¨æ–‡ SHA256
    meta_hash: str         # Frontmatter SHA256
    frontmatter: Dict      # å…ƒæ•°æ®
    content: str           # æ­£æ–‡
    updated_at: float      # æ–‡ä»¶ mtime
```

**å…³é”®å®ç°**:

- ä½¿ç”¨ `asyncio.to_thread()` é¿å…é˜»å¡
- `python-frontmatter` åº“è§£æ
- SHA256 å“ˆå¸Œä¿è¯å”¯ä¸€æ€§

**æ€§èƒ½ä¼˜åŒ–**:

```python
# å¼‚æ­¥æ–‡ä»¶è¯»å–
raw_content = await asyncio.to_thread(
    full_path.read_text,
    encoding="utf-8"
)

# å“ˆå¸Œè®¡ç®—
content_hash = hashlib.sha256(raw_content.encode()).hexdigest()
meta_hash = hashlib.sha256(
    json.dumps(metadata, sort_keys=True).encode()
).hexdigest()
```

---

### 4. `mapper.py` - å­—æ®µæ˜ å°„å±‚

**èŒè´£**:

- Frontmatter åˆ° Post æ¨¡å‹çš„å­—æ®µè½¬æ¢
- é»˜è®¤å€¼å¤„ç†
- ç±»å‹è½¬æ¢å’ŒéªŒè¯

**æ˜ å°„ç­–ç•¥**:

| ç­–ç•¥     | è¯´æ˜                 | ç¤ºä¾‹                            |
| -------- | -------------------- | ------------------------------- |
| ç›´æ¥æ˜ å°„ | å­—æ®µåç›¸åŒ           | `title` â†’ `title`               |
| åˆ«åæ˜ å°„ | å¤šä¸ªå­—æ®µæ˜ å°„åˆ°åŒä¸€ä¸ª | `summary`/`excerpt` â†’ `excerpt` |
| é»˜è®¤å€¼   | å­—æ®µç¼ºå¤±æ—¶ä½¿ç”¨é»˜è®¤å€¼ | `status` é»˜è®¤ `PUBLISHED`       |
| ç±»å‹è½¬æ¢ | å­—ç¬¦ä¸²è½¬æšä¸¾/æ—¥æœŸ    | `"draft"` â†’ `PostStatus.DRAFT`  |
| å¼•ç”¨è§£æ | å­—ç¬¦ä¸²è½¬ UUID        | `"admin"` â†’ `UUID(...)`         |

**å…³é”®æ–¹æ³•**:

```python
async def map_to_post(scanned: ScannedPost) -> Dict[str, Any]:
    # 1. è§£æä½œè€…ï¼ˆå¿…å¡«ï¼‰
    author_id = await self.author_resolver.resolve(meta.get("author"))

    # 2. è§£æå°é¢ï¼ˆå¯é€‰ï¼‰
    cover_media_id = await self.cover_resolver.resolve(meta.get("cover"))

    # 3. æ„å»ºå­—æ®µæ˜ å°„
    return {
        "title": meta.get("title", Path(file_path).stem),
        "author_id": author_id,
        "cover_media_id": cover_media_id,
        "status": self._resolve_status(meta),
        "published_at": self._resolve_date(meta),
        ...
    }
```

---

### 5. `resolvers.py` - å¼•ç”¨è§£æå±‚

**èŒè´£**:

- å°†å­—ç¬¦ä¸²å¼•ç”¨è½¬æ¢ä¸ºæ•°æ®åº“ ID
- å¤„ç†å¤šç§æŸ¥è¯¢ç­–ç•¥
- é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•

**è§£æå™¨ç±»å‹**:

#### AuthorResolver

```python
async def resolve(author_value: str) -> UUID:
    # 1. å°è¯•ä½œä¸º UUID è§£æ
    if is_uuid_format(author_value):
        user = await query_by_id(author_value)
        if user:
            return user.id

    # 2. ä½œä¸ºç”¨æˆ·åæŸ¥è¯¢
    user = await query_by_username(author_value)
    if user:
        return user.id

    # 3. æœªæ‰¾åˆ°åˆ™æŠ›å‡ºå¼‚å¸¸
    raise GitOpsSyncError(f"Author not found: {author_value}")
```

#### CoverResolver

```python
async def resolve(cover_path: str) -> Optional[UUID]:
    # 1. ç²¾ç¡®åŒ¹é… file_path
    media = await query_by_path(cover_path)
    if media:
        return media.id

    # 2. åŒ¹é… original_filename
    filename = Path(cover_path).name
    media = await query_by_filename(filename)
    if media:
        return media.id

    # 3. åç¼€åŒ¹é… file_path
    media = await query_by_path_suffix(filename)
    if media:
        return media.id

    # 4. æœªæ‰¾åˆ°è¿”å› Noneï¼ˆä¸æŠ›å‡ºå¼‚å¸¸ï¼‰
    return None
```

**è®¾è®¡åŸåˆ™**:

- **å¤šç­–ç•¥æŸ¥è¯¢**: æé«˜åŒ¹é…æˆåŠŸç‡
- **å®¹é”™æ€§**: å°é¢å›¾æ‰¾ä¸åˆ°ä¸å½±å“åŒæ­¥
- **æ—¥å¿—è®°å½•**: è®°å½•åŒ¹é…æ–¹å¼å’Œå¤±è´¥åŸå› 

---

### 6. `git_client.py` - Git æ“ä½œå±‚

**èŒè´£**:

- æ‰§è¡Œ Git å‘½ä»¤
- éé˜»å¡å¼‚æ­¥è°ƒç”¨
- é”™è¯¯å¤„ç†

**å·²å®ç°æ–¹æ³•**:

| æ–¹æ³•                  | åŠŸèƒ½             | çŠ¶æ€      |
| --------------------- | ---------------- | --------- |
| `pull()`              | æ‹‰å–æœ€æ–°ä»£ç      | âœ… å·²å®ç° |
| `get_current_hash()`  | è·å–å½“å‰ commit  | âœ… å·²å®ç° |
| `get_changed_files()` | è·å–å˜æ›´æ–‡ä»¶åˆ—è¡¨ | âœ… å·²å®ç° |
| `get_file_status()`   | å·¥ä½œåŒºçŠ¶æ€       | âœ… å·²å®ç° |

**å®ç°ç»†èŠ‚**:

```python
async def run(self, *args: str) -> Tuple[int, str, str]:
    cmd = ["git"] + list(args)
    process = await asyncio.create_subprocess_exec(
        *cmd,
        cwd=self.repo_path,
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE,
    )
    stdout, stderr = await process.communicate()
    return process.returncode, stdout.decode(), stderr.decode()
```

**æœªæ¥é›†æˆ**:
å½“å‰ `sync_all()` ä¸ºå…¨é‡åŒæ­¥ï¼Œæœªå……åˆ†åˆ©ç”¨ GitClientã€‚
è®¡åˆ’åœ¨å¢é‡åŒæ­¥æ—¶é›†æˆï¼š

```python
# æœªæ¥ä»£ç ç¤ºä¾‹
before_hash = await git_client.get_current_hash()
await git_client.pull()
after_hash = await git_client.get_current_hash()

if before_hash != after_hash:
    changed_files = await git_client.get_changed_files(before_hash)
    # åªåŒæ­¥å˜æ›´çš„æ–‡ä»¶
    for file in changed_files:
        await sync_single_file(file)
```

---

### 7. `exceptions.py` - å¼‚å¸¸å®šä¹‰

**å¼‚å¸¸å±‚æ¬¡ç»“æ„**:

```mermaid
classDiagram
    BaseAppException <|-- GitOpsError
    GitOpsError <|-- GitOpsConfigurationError
    GitOpsError <|-- GitOpsSyncError

    class BaseAppException {
        +message: str
        +status_code: int
        +error_code: str
        +details: dict
    }

    class GitOpsError {
        <<abstract>>
    }

    class GitOpsConfigurationError {
        +status_code: 500
        +error_code: GITOPS_CONFIG_ERROR
        +__init__(message)
    }

    class GitOpsSyncError {
        +status_code: 400
        +error_code: GITOPS_SYNC_ERROR
        +__init__(message, detail)
    }
```

**ä½¿ç”¨åœºæ™¯**:

| å¼‚å¸¸                       | ä½¿ç”¨åœºæ™¯             | å¤„ç†æ–¹å¼                 |
| -------------------------- | -------------------- | ------------------------ |
| `GitOpsConfigurationError` | é…ç½®é”™è¯¯ã€ç›®å½•ä¸å­˜åœ¨ | ä¸­æ–­åŒæ­¥ï¼Œè¿”å› 500       |
| `GitOpsSyncError`          | å­—æ®µç¼ºå¤±ã€å¼•ç”¨ä¸å­˜åœ¨ | è®°å½•é”™è¯¯ï¼Œç»§ç»­åŒæ­¥       |
| `GitError`                 | Git å‘½ä»¤å¤±è´¥         | è®°å½•è­¦å‘Šï¼Œé™çº§ä¸ºæœ¬åœ°åŒæ­¥ |

---

## ğŸ”— ä¸å…¶ä»–æ¨¡å—çš„äº¤äº’

### ä¾èµ–å…³ç³»å›¾

```mermaid
graph TD
    GitOps[git_ops] --> Posts[posts]
    GitOps --> Users[users]
    GitOps --> Media[media]
    GitOps --> Core[core]

    Posts --> DB[(Database)]
    Users --> DB
    Media --> DB
    Core --> Settings[settings]

    GitOps -.->|æœªæ¥é›†æˆ| Git[Git Repository]

    subgraph "posts æ¨¡å—"
        Posts --> PostService[service.py]
        PostService --> PostCRUD[crud.py]
        PostService --> PostSchema[schema.py]
        PostService --> PostModel[model.py]
    end

    subgraph "users æ¨¡å—"
        Users --> UserDeps[dependencies.py]
        UserDeps --> Auth[è®¤è¯]
        Users --> UserModel[model.py]
    end

    subgraph "media æ¨¡å—"
        Media --> MediaModel[model.py]
        Media --> MediaSchema[schema.py]
    end

    subgraph "core æ¨¡å—"
        Core --> Config[config.py]
        Core --> Exceptions[exceptions.py]
        Core --> DBSession[db.py]
    end
```

### è°ƒç”¨é“¾åˆ†æ

```
HTTP Request
    â†“
FastAPI Router (router.py)
    â†“ Depends(get_current_adminuser) â† users.dependencies
    â†“ Depends(get_async_session) â† core.db
    â†“
GitOpsService.sync_all() (service.py)
    â†“
    â”œâ”€â†’ GitClient.pull() (git_client.py)
    â”‚       â””â”€â†’ asyncio.create_subprocess_exec
    â”‚
    â”œâ”€â†’ MDXScanner.scan_all() (scanner.py)
    â”‚       â”œâ”€â†’ Path.glob() â†’ æ–‡ä»¶ç³»ç»Ÿ
    â”‚       â””â”€â†’ frontmatter.loads() â†’ è§£æ
    â”‚
    â”œâ”€â†’ FrontmatterMapper.map_to_post() (mapper.py)
    â”‚       â”œâ”€â†’ AuthorResolver.resolve() (resolvers.py)
    â”‚       â”‚       â””â”€â†’ SELECT User â†’ PostgreSQL
    â”‚       â””â”€â†’ CoverResolver.resolve() (resolvers.py)
    â”‚               â””â”€â†’ SELECT MediaFile â†’ PostgreSQL
    â”‚
    â””â”€â†’ PostService CRUD (posts.service)
            â”œâ”€â†’ create_post() â†’ INSERT Post
            â”œâ”€â†’ update_post() â†’ UPDATE Post
            â””â”€â†’ delete_post() â†’ DELETE Post
                    â†“
                PostgreSQL
```

---

## ğŸ“Š æ•°æ®æ¨¡å‹å…³ç³»

### Post æ¨¡å‹å…³é”®å­—æ®µ

```mermaid
erDiagram
    POST {
        uuid id PK
        string title
        string slug UK
        string content_mdx
        string excerpt
        enum status
        datetime published_at
        string source_path UK "GitOps å…³é”®å­—æ®µ"
        uuid author_id FK
        uuid cover_media_id FK
        datetime created_at
        datetime updated_at
    }

    USER {
        uuid id PK
        string username UK
        enum role
    }

    MEDIA_FILE {
        uuid id PK
        string file_path UK
        string original_filename
        jsonb thumbnails
    }

    POST ||--o{ USER : "author"
    POST ||--o{ MEDIA_FILE : "cover"

    note "source_path: æ ‡è¯†æ–‡ä»¶ç³»ç»Ÿæ¥æº<br>å”¯ä¸€çº¦æŸé˜²æ­¢é‡å¤åŒæ­¥"
```

### åŒæ­¥çŠ¶æ€åˆ¤æ–­

| åœºæ™¯     | source_path (DB) | file_path (FS) | æ“ä½œ                  |
| -------- | ---------------- | -------------- | --------------------- |
| æ–°æ–‡ä»¶   | NULL / ä¸å­˜åœ¨    | å­˜åœ¨           | CREATE                |
| æ›´æ–°æ–‡ä»¶ | å­˜åœ¨             | å­˜åœ¨           | UPDATE                |
| åˆ é™¤æ–‡ä»¶ | å­˜åœ¨             | ä¸å­˜åœ¨         | DELETE                |
| æ‰‹åŠ¨åˆ›å»º | NULL             | -              | å¿½ç•¥ï¼ˆä¸å‚ä¸ GitOpsï¼‰ |

**å…³é”®è®¾è®¡**:

- `source_path` ä¸º `NULL` çš„æ–‡ç« ä¸å‚ä¸ GitOps åŒæ­¥
- `source_path` æœ‰å”¯ä¸€çº¦æŸï¼Œé˜²æ­¢é‡å¤åŒæ­¥
- åˆ é™¤æ“ä½œåªåˆ é™¤ `source_path` ä¸ä¸º `NULL` çš„æ–‡ç« 

---

## âš¡ æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### å½“å‰å®ç°

1. **å¼‚æ­¥ I/O**

   - `asyncio.to_thread()` æ–‡ä»¶è¯»å–
   - `AsyncSession` æ•°æ®åº“æ“ä½œ
   - `asyncio.create_subprocess_exec` Git å‘½ä»¤

2. **å•æ¬¡æ•°æ®åº“æŸ¥è¯¢**

   - ä¸€æ¬¡æŸ¥è¯¢è·å–æ‰€æœ‰ GitOps æ–‡ç« 
   - å†…å­˜ä¸­æ„å»ºæ˜ å°„è¡¨ï¼ˆO(1) æŸ¥æ‰¾ï¼‰

3. **é”™è¯¯éš”ç¦»**
   - å•æ–‡ä»¶å¤±è´¥ä¸å½±å“æ•´ä½“æµç¨‹
   - ä½¿ç”¨ try-except æ•è·å¼‚å¸¸

### æ€§èƒ½ç“¶é¢ˆåˆ†æ

| æ“ä½œ          | å½“å‰å®ç° | æ—¶é—´å¤æ‚åº¦ | ç“¶é¢ˆ       |
| ------------- | -------- | ---------- | ---------- |
| æ–‡ä»¶æ‰«æ      | å…¨é‡æ‰«æ | O(n)       | I/O å¯†é›†   |
| æ–‡ä»¶è§£æ      | ä¸²è¡Œå¤„ç† | O(n)       | CPU å¯†é›†   |
| æ•°æ®åº“æŸ¥è¯¢    | å•æ¬¡æŸ¥è¯¢ | O(1)       | ç½‘ç»œå»¶è¿Ÿ   |
| æ–‡ç« åˆ›å»º/æ›´æ–° | ä¸²è¡Œå¤„ç† | O(n)       | æ•°æ®åº“å†™å…¥ |

**n = æ–‡ä»¶æ•°é‡**

### ä¼˜åŒ–å»ºè®®

#### 1. å¢é‡åŒæ­¥ï¼ˆé‡è¦ï¼‰â­â­â­

**å½“å‰é—®é¢˜**: æ¯æ¬¡åŒæ­¥æ‰«ææ‰€æœ‰æ–‡ä»¶ï¼Œå³ä½¿åªä¿®æ”¹äº†ä¸€ä¸ªæ–‡ä»¶

**ä¼˜åŒ–æ–¹æ¡ˆ**:

```python
# è®°å½•ä¸Šæ¬¡åŒæ­¥çš„ commit hash
last_sync_hash = await get_last_sync_hash()

# Git pull
await git_client.pull()
current_hash = await git_client.get_current_hash()

# åªå¤„ç†å˜æ›´çš„æ–‡ä»¶
if last_sync_hash != current_hash:
    changed_files = await git_client.get_changed_files(last_sync_hash)

    for file in changed_files:
        if file.endswith(('.md', '.mdx')):
            await sync_single_file(file)

    # æ›´æ–° last_sync_hash
    await save_last_sync_hash(current_hash)
```

**é¢„æœŸæ”¶ç›Š**:

- ğŸš€ å‡å°‘ 95% æ–‡ä»¶æ‰«ææ—¶é—´
- ğŸ’¾ é™ä½ CPU å’Œå†…å­˜å ç”¨
- âš¡ åŒæ­¥é€Ÿåº¦æå‡ 10-100 å€ï¼ˆå–å†³äºå˜æ›´æ–‡ä»¶æ•°é‡ï¼‰

**å®ç°å¤æ‚åº¦**: ä¸­ç­‰

---

#### 2. å¹¶å‘å¤„ç† â­â­

**å½“å‰é—®é¢˜**: æ–‡ä»¶æ‰«æå’Œå¤„ç†éƒ½æ˜¯ä¸²è¡Œçš„

**ä¼˜åŒ–æ–¹æ¡ˆ**:

```python
# å¹¶å‘æ‰«ææ–‡ä»¶
async def scan_all_concurrent(files: List[Path]) -> List[ScannedPost]:
    tasks = [scanner.scan_file(str(f.relative_to(content_root))) for f in files]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # è¿‡æ»¤æ‰å¤±è´¥çš„æ–‡ä»¶
    return [r for r in results if isinstance(r, ScannedPost)]

# å¹¶å‘å¤„ç†æ–‡ä»¶
async def process_files_concurrent(files: List[ScannedPost]):
    tasks = [process_single_file(f) for f in files]
    await asyncio.gather(*tasks, return_exceptions=True)
```

**é¢„æœŸæ”¶ç›Š**:

- âš¡ I/O å¯†é›†å‹ä»»åŠ¡æé€Ÿ 3-5 å€
- ğŸ”„ å……åˆ†åˆ©ç”¨å¤šæ ¸ CPU

**æ³¨æ„äº‹é¡¹**:

- éœ€è¦å¤„ç†å¹¶å‘æ•°æ®åº“å†™å…¥
- å¯èƒ½éœ€è¦ä½¿ç”¨ä¿¡å·é‡é™åˆ¶å¹¶å‘æ•°

**å®ç°å¤æ‚åº¦**: ä¸­ç­‰

---

#### 3. ç¼“å­˜ä¼˜åŒ– â­

**å½“å‰é—®é¢˜**: æ¯æ¬¡éƒ½é‡æ–°è®¡ç®—æ–‡ä»¶å“ˆå¸Œ

**ä¼˜åŒ–æ–¹æ¡ˆ**:

```python
# å†…å­˜ç¼“å­˜æ–‡ä»¶å“ˆå¸Œ
cache = {
    "file.mdx": {
        "hash": "abc123...",
        "mtime": 1234567890.0
    }
}

async def scan_file_with_cache(file_path: str) -> Optional[ScannedPost]:
    full_path = content_root / file_path
    current_mtime = full_path.stat().st_mtime

    # æ£€æŸ¥ç¼“å­˜
    if file_path in cache and cache[file_path]["mtime"] == current_mtime:
        logger.debug(f"Cache hit: {file_path}")
        return cache[file_path]["scanned"]

    # æ‰«ææ–‡ä»¶
    scanned = await scan_file(file_path)

    # æ›´æ–°ç¼“å­˜
    cache[file_path] = {
        "mtime": current_mtime,
        "scanned": scanned
    }

    return scanned
```

**é¢„æœŸæ”¶ç›Š**:

- ğŸš€ è·³è¿‡æœªä¿®æ”¹æ–‡ä»¶çš„æ‰«æ
- ğŸ’¾ å‡å°‘æ–‡ä»¶ I/O

**å®ç°å¤æ‚åº¦**: ä½

---

#### 4. æ‰¹é‡æ“ä½œ â­â­

**å½“å‰é—®é¢˜**: æ–‡ç« åˆ›å»º/æ›´æ–°æ˜¯é€ä¸ªæ‰§è¡Œçš„

**ä¼˜åŒ–æ–¹æ¡ˆ**:

```python
# æ‰¹é‡æ’å…¥ï¼ˆä½¿ç”¨ SQLAlchemy Coreï¼‰
from sqlalchemy import insert

async def batch_create_posts(posts: List[Dict]):
    stmt = insert(Post).values(posts)
    await session.execute(stmt)
    await session.commit()

# æ‰¹é‡æ›´æ–°
from sqlalchemy import update

async def batch_update_posts(updates: List[Tuple[UUID, Dict]]):
    for post_id, data in updates:
        stmt = update(Post).where(Post.id == post_id).values(**data)
        await session.execute(stmt)
    await session.commit()
```

**é¢„æœŸæ”¶ç›Š**:

- âš¡ å‡å°‘æ•°æ®åº“å¾€è¿”æ¬¡æ•°
- ğŸ”„ æå‡å†™å…¥æ€§èƒ½ 2-3 å€

**æ³¨æ„äº‹é¡¹**:

- éœ€è¦å¤„ç†æ‰¹é‡æ“ä½œçš„é”™è¯¯
- å¯èƒ½éœ€è¦åˆ†æ‰¹å¤„ç†ï¼ˆé¿å…å•æ¬¡æ“ä½œè¿‡å¤§ï¼‰

**å®ç°å¤æ‚åº¦**: é«˜

---

#### 5. æ•°æ®åº“ç´¢å¼•ä¼˜åŒ– â­

**å½“å‰é—®é¢˜**: æŸ¥è¯¢å¯èƒ½ç¼ºå°‘ç´¢å¼•

**ä¼˜åŒ–æ–¹æ¡ˆ**:

```sql
-- ä¸º source_path æ·»åŠ ç´¢å¼•ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
CREATE INDEX idx_post_source_path ON posts(source_path) WHERE source_path IS NOT NULL;

-- ä¸º author_id æ·»åŠ ç´¢å¼•
CREATE INDEX idx_post_author_id ON posts(author_id);

-- ä¸º cover_media_id æ·»åŠ ç´¢å¼•
CREATE INDEX idx_post_cover_media_id ON posts(cover_media_id);

-- ä¸º MediaFile çš„æŸ¥è¯¢å­—æ®µæ·»åŠ ç´¢å¼•
CREATE INDEX idx_media_file_path ON media_files(file_path);
CREATE INDEX idx_media_original_filename ON media_files(original_filename);
```

**é¢„æœŸæ”¶ç›Š**:

- âš¡ æŸ¥è¯¢é€Ÿåº¦æå‡ 10-100 å€
- ğŸ’¾ å‡å°‘æ•°æ®åº“è´Ÿè½½

**å®ç°å¤æ‚åº¦**: ä½

---

### æ€§èƒ½å¯¹æ¯”ï¼ˆé¢„ä¼°ï¼‰

| åœºæ™¯                  | å½“å‰å®ç° | å¢é‡åŒæ­¥ | å¢é‡+å¹¶å‘ | å¢é‡+å¹¶å‘+ç¼“å­˜ |
| --------------------- | -------- | -------- | --------- | -------------- |
| 100 ä¸ªæ–‡ä»¶ï¼Œ1 ä¸ªå˜æ›´  | 10s      | 0.5s     | 0.3s      | 0.2s           |
| 100 ä¸ªæ–‡ä»¶ï¼Œ10 ä¸ªå˜æ›´ | 10s      | 2s       | 1s        | 0.8s           |
| 100 ä¸ªæ–‡ä»¶ï¼Œå…¨é‡å˜æ›´  | 10s      | 10s      | 3s        | 2.5s           |
| 1000 ä¸ªæ–‡ä»¶ï¼Œ1 ä¸ªå˜æ›´ | 100s     | 0.5s     | 0.3s      | 0.2s           |

**ç»“è®º**: å¢é‡åŒæ­¥æ˜¯æœ€é‡è¦çš„ä¼˜åŒ–ï¼Œå¯ä»¥å¸¦æ¥ 10-100 å€çš„æ€§èƒ½æå‡

---

## ğŸ§© æ‰©å±•ç‚¹è®¾è®¡

### 1. è‡ªå®šä¹‰å­—æ®µæ˜ å°„

**åœºæ™¯**: ä¸åŒé¡¹ç›®å¯èƒ½æœ‰ä¸åŒçš„ Frontmatter å­—æ®µ

**å®ç°æ–¹å¼**:

```python
class CustomMapper(FrontmatterMapper):
    async def map_to_post(self, scanned: ScannedPost) -> Dict:
        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•è·å–åŸºç¡€æ˜ å°„
        result = await super().map_to_post(scanned)

        # æ·»åŠ è‡ªå®šä¹‰æ˜ å°„
        meta = scanned.frontmatter
        result["custom_field"] = meta.get("my_custom_field")

        return result

# åœ¨ Service ä¸­æ³¨å…¥
service = GitOpsService(session, mapper=CustomMapper(session))
```

---

### 2. åŒæ­¥é’©å­ï¼ˆHooksï¼‰

**åœºæ™¯**: åœ¨åŒæ­¥çš„ä¸åŒé˜¶æ®µæ‰§è¡Œè‡ªå®šä¹‰é€»è¾‘

**å®ç°æ–¹å¼**:

```python
class SyncHooks:
    async def before_sync(self, files: List[str]):
        """åŒæ­¥å‰éªŒè¯"""
        logger.info(f"About to sync {len(files)} files")

    async def after_create(self, post: Post):
        """åˆ›å»ºåé€šçŸ¥"""
        await send_webhook(post)
        await clear_cache(post.id)

    async def after_update(self, post: Post):
        """æ›´æ–°åå¤„ç†"""
        await invalidate_cache(post.id)

    async def after_delete(self, post_id: UUID):
        """åˆ é™¤åæ¸…ç†"""
        await cleanup_related_data(post_id)

    async def after_sync(self, stats: SyncStats):
        """åŒæ­¥å®Œæˆåå¤„ç†"""
        await send_notification(stats)
        await update_metrics(stats)

# åœ¨ Service ä¸­ä½¿ç”¨
service = GitOpsService(session, hooks=SyncHooks())
```

---

### 3. å¤šä»“åº“æ”¯æŒ

**åœºæ™¯**: æ”¯æŒä»å¤šä¸ª Git ä»“åº“åŒæ­¥å†…å®¹

**å®ç°æ–¹å¼**:

```python
repos = [
    {
        "path": "content/blog",
        "category": "blog",
        "author": "admin"
    },
    {
        "path": "content/docs",
        "category": "documentation",
        "author": "doc_writer"
    },
]

for repo in repos:
    service = GitOpsService(
        session,
        content_dir=repo["path"]
    )

    # è®¾ç½®é»˜è®¤åˆ†ç±»å’Œä½œè€…
    default_user = await get_user(repo["author"])
    stats = await service.sync_all(default_user=default_user)

    # ä¸ºæ‰€æœ‰æ–‡ç« è®¾ç½®åˆ†ç±»
    await set_category_for_repo(repo["category"], stats.added)
```

---

### 4. è‡ªå®šä¹‰æ–‡ä»¶è¿‡æ»¤

**åœºæ™¯**: åªåŒæ­¥ç‰¹å®šç±»å‹æˆ–è·¯å¾„çš„æ–‡ä»¶

**å®ç°æ–¹å¼**:

```python
class FilteredScanner(MDXScanner):
    def __init__(self, content_root: Path, filters: List[Callable]):
        super().__init__(content_root)
        self.filters = filters

    async def scan_all(self, glob_patterns: List[str] = None) -> List[ScannedPost]:
        results = await super().scan_all(glob_patterns)

        # åº”ç”¨è¿‡æ»¤å™¨
        for filter_func in self.filters:
            results = [r for r in results if filter_func(r)]

        return results

# ä½¿ç”¨ç¤ºä¾‹
def only_published(scanned: ScannedPost) -> bool:
    return scanned.frontmatter.get("published", True)

def only_blog_posts(scanned: ScannedPost) -> bool:
    return scanned.file_path.startswith("blog/")

scanner = FilteredScanner(
    content_root,
    filters=[only_published, only_blog_posts]
)
```

---

## ğŸ” å®‰å…¨æ€§è®¾è®¡

### å¨èƒæ¨¡å‹

| å¨èƒ             | æè¿°                      | ç¼“è§£æªæ–½              | çŠ¶æ€ |
| ---------------- | ------------------------- | --------------------- | ---- |
| è·¯å¾„éå†æ”»å‡»     | æ¶æ„æ–‡ä»¶è·¯å¾„è®¿é—®ç³»ç»Ÿæ–‡ä»¶  | é™åˆ¶åœ¨ CONTENT_DIR å†… | âœ…   |
| æ¶æ„ Frontmatter | æ³¨å…¥æ¶æ„æ•°æ®åˆ°æ•°æ®åº“      | Pydantic éªŒè¯         | âœ…   |
| SQL æ³¨å…¥         | é€šè¿‡ Frontmatter æ³¨å…¥ SQL | SQLModel ORM          | âœ…   |
| æƒé™æå‡         | éç®¡ç†å‘˜è§¦å‘åŒæ­¥          | ç®¡ç†å‘˜è®¤è¯            | âœ…   |
| DDoS åŒæ­¥        | é¢‘ç¹è§¦å‘åŒæ­¥å¯¼è‡´èµ„æºè€—å°½  | æœªå®ç°é€Ÿç‡é™åˆ¶        | âš ï¸   |
| æ•æ„Ÿä¿¡æ¯æ³„éœ²     | æ—¥å¿—ä¸­åŒ…å«æ•æ„Ÿä¿¡æ¯        | æ—¥å¿—è„±æ•              | âš ï¸   |
| å¤§æ–‡ä»¶æ”»å‡»       | ä¸Šä¼ è¶…å¤§æ–‡ä»¶å¯¼è‡´å†…å­˜æº¢å‡º  | æœªå®ç°æ–‡ä»¶å¤§å°é™åˆ¶    | âš ï¸   |

### æƒé™çŸ©é˜µ

| æ“ä½œ             | åŒ¿å | æ™®é€šç”¨æˆ· | ç®¡ç†å‘˜ | è¶…çº§ç®¡ç†å‘˜ |
| ---------------- | ---- | -------- | ------ | ---------- |
| è§¦å‘åŒæ­¥         | âŒ   | âŒ       | âœ…     | âœ…         |
| æŸ¥çœ‹åŒæ­¥å†å²     | âŒ   | âŒ       | âœ…     | âœ…         |
| é…ç½® CONTENT_DIR | âŒ   | âŒ       | âŒ     | âœ…         |
| ä¿®æ”¹åŒæ­¥è®¾ç½®     | âŒ   | âŒ       | âŒ     | âœ…         |

### å®‰å…¨æœ€ä½³å®è·µ

1. **è¾“å…¥éªŒè¯**

   ```python
   # éªŒè¯æ–‡ä»¶è·¯å¾„
   def validate_file_path(file_path: str) -> bool:
       # ç¡®ä¿è·¯å¾„åœ¨ content_root å†…
       full_path = (content_root / file_path).resolve()
       return full_path.is_relative_to(content_root)
   ```

2. **æ—¥å¿—è„±æ•**

   ```python
   def sanitize_log(message: str) -> str:
       # ç§»é™¤æ•æ„Ÿä¿¡æ¯
       message = re.sub(r'password=\S+', 'password=***', message)
       message = re.sub(r'token=\S+', 'token=***', message)
       return message
   ```

3. **é€Ÿç‡é™åˆ¶**

   ```python
   from fastapi_limiter import FastAPILimiter
   from fastapi_limiter.depends import RateLimiter

   @router.post("/sync", dependencies=[Depends(RateLimiter(times=10, minutes=1))])
   async def trigger_sync(...):
       ...
   ```

4. **æ–‡ä»¶å¤§å°é™åˆ¶**

   ```python
   MAX_FILE_SIZE = 10 * 1024 * 1024  # 10MB

   async def scan_file(file_path: str) -> Optional[ScannedPost]:
       full_path = content_root / file_path

       # æ£€æŸ¥æ–‡ä»¶å¤§å°
       if full_path.stat().st_size > MAX_FILE_SIZE:
           logger.warning(f"File too large: {file_path}")
           return None

       ...
   ```

---

## ğŸ“ˆ ç›‘æ§ä¸å¯è§‚æµ‹æ€§

### å»ºè®®æ·»åŠ çš„æŒ‡æ ‡

```python
from prometheus_client import Counter, Histogram, Gauge

# åŒæ­¥æ¬¡æ•°
gitops_sync_total = Counter(
    'gitops_sync_total',
    'Total number of sync operations',
    ['status']  # success, error
)

# åŒæ­¥è€—æ—¶
gitops_sync_duration_seconds = Histogram(
    'gitops_sync_duration_seconds',
    'Time spent in sync operation',
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0]
)

# å¤„ç†çš„æ–‡ä»¶æ•°
gitops_files_processed_total = Counter(
    'gitops_files_processed_total',
    'Total number of files processed',
    ['operation']  # added, updated, deleted, skipped
)

# é”™è¯¯æ•°
gitops_errors_total = Counter(
    'gitops_errors_total',
    'Total number of errors',
    ['error_type']  # config, sync, validation
)

# æœ€ååŒæ­¥æ—¶é—´
gitops_last_sync_timestamp = Gauge(
    'gitops_last_sync_timestamp',
    'Timestamp of last successful sync'
)

# ä½¿ç”¨ç¤ºä¾‹
@router.post("/sync")
async def trigger_sync(...):
    with gitops_sync_duration_seconds.time():
        try:
            stats = await service.sync_all(...)
            gitops_sync_total.labels(status='success').inc()
            gitops_files_processed_total.labels(operation='added').inc(len(stats.added))
            gitops_files_processed_total.labels(operation='updated').inc(len(stats.updated))
            gitops_files_processed_total.labels(operation='deleted').inc(len(stats.deleted))
            gitops_last_sync_timestamp.set(time.time())
            return stats
        except Exception as e:
            gitops_sync_total.labels(status='error').inc()
            gitops_errors_total.labels(error_type='sync').inc()
            raise
```

### æ—¥å¿—çº§åˆ«

| äº‹ä»¶          | çº§åˆ«    | ç¤ºä¾‹                                            |
| ------------- | ------- | ----------------------------------------------- |
| åŒæ­¥å¼€å§‹      | INFO    | `Starting GitOps sync...`                       |
| æ–‡ä»¶æ‰«æå®Œæˆ  | INFO    | `Scanned 42 files.`                             |
| Git Pull æˆåŠŸ | INFO    | `Git pull result: Already up to date.`          |
| Git Pull å¤±è´¥ | WARNING | `Git pull skipped/failed: Not a git repository` |
| æ–‡ä»¶å¤„ç†å¤±è´¥  | WARNING | `Failed to sync file.mdx: ValidationError`      |
| ä½œè€…ä¸å­˜åœ¨    | WARNING | `Author not found: username`                    |
| å°é¢å›¾æœªæ‰¾åˆ°  | WARNING | `Cover image not found: cover.jpg`              |
| é…ç½®é”™è¯¯      | ERROR   | `CONTENT_DIR not found: /path/to/content`       |
| æœªé¢„æœŸçš„é”™è¯¯  | ERROR   | `Unexpected error syncing file.mdx: ...`        |
| åŒæ­¥å®Œæˆ      | INFO    | `Sync completed in 1.23s: +2 ~3 -1`             |

### åˆ†å¸ƒå¼è¿½è¸ª

```python
from opentelemetry import trace
from opentelemetry.trace import Status, StatusCode

tracer = trace.get_tracer(__name__)

async def sync_all(self, default_user: User = None) -> SyncStats:
    with tracer.start_as_current_span("gitops.sync_all") as span:
        span.set_attribute("user.id", str(default_user.id))

        try:
            # Git Pull
            with tracer.start_as_current_span("gitops.git_pull"):
                await self.git_client.pull()

            # æ‰«ææ–‡ä»¶
            with tracer.start_as_current_span("gitops.scan_files") as scan_span:
                scanned_posts = await self.scanner.scan_all()
                scan_span.set_attribute("files.count", len(scanned_posts))

            # å¤„ç†æ–‡ä»¶
            with tracer.start_as_current_span("gitops.process_files"):
                ...

            span.set_status(Status(StatusCode.OK))
            return stats
        except Exception as e:
            span.set_status(Status(StatusCode.ERROR, str(e)))
            span.record_exception(e)
            raise
```

---

## ğŸ”„ éƒ¨ç½²å»ºè®®

### æ–¹å¼ä¸€ï¼šå®šæ—¶ä»»åŠ¡

```python
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger

scheduler = AsyncIOScheduler()

async def sync_task():
    async for session in get_async_session():
        service = GitOpsService(session)
        admin = await get_admin_user(session)
        stats = await service.sync_all(default_user=admin)
        logger.info(f"Scheduled sync completed: {stats}")

# æ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡
scheduler.add_job(
    sync_task,
    CronTrigger(hour='*/1'),
    id='gitops_sync',
    name='GitOps Sync',
    replace_existing=True
)

scheduler.start()
```

### æ–¹å¼äºŒï¼šWebhook

```python
import hmac
import hashlib

@router.post("/webhook/github")
async def github_webhook(
    request: Request,
    x_hub_signature_256: str = Header(...),
    session: AsyncSession = Depends(get_async_session),
):
    # éªŒè¯ç­¾å
    payload = await request.body()
    expected = hmac.new(
        settings.WEBHOOK_SECRET.encode(),
        payload,
        hashlib.sha256
    ).hexdigest()

    if not hmac.compare_digest(f"sha256={expected}", x_hub_signature_256):
        raise HTTPException(401, "Invalid signature")

    # è§£æ payload
    data = await request.json()

    # åªå¤„ç† push äº‹ä»¶
    if data.get("ref") == "refs/heads/main":
        # è§¦å‘åŒæ­¥
        service = GitOpsService(session)
        admin = await get_admin_user(session)
        stats = await service.sync_all(default_user=admin)

        return {"status": "triggered", "stats": stats}

    return {"status": "ignored"}
```

### æ–¹å¼ä¸‰ï¼šæ–‡ä»¶ç›‘å¬

```python
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

class SyncHandler(FileSystemEventHandler):
    def __init__(self, service: GitOpsService):
        self.service = service

    def on_modified(self, event):
        if event.src_path.endswith(('.md', '.mdx')):
            asyncio.create_task(self.sync_file(event.src_path))

    async def sync_file(self, file_path: str):
        logger.info(f"File modified: {file_path}")
        # è§¦å‘åŒæ­¥
        await self.service.sync_single_file(file_path)

# å¯åŠ¨ç›‘å¬
observer = Observer()
observer.schedule(
    SyncHandler(service),
    path='content/',
    recursive=True
)
observer.start()
```

---

## ğŸ“ æ€»ç»“

### æ¶æ„äº®ç‚¹

âœ… **å…³æ³¨ç‚¹åˆ†ç¦»** - æ¸…æ™°çš„åˆ†å±‚æ¶æ„ï¼Œæ¯ä¸ªæ¨¡å—èŒè´£å•ä¸€
âœ… **å¼‚æ­¥ä¼˜å…ˆ** - å…¨å¼‚æ­¥ I/O è®¾è®¡ï¼Œå……åˆ†åˆ©ç”¨ asyncio
âœ… **é”™è¯¯éš”ç¦»** - å•æ–‡ä»¶å¤±è´¥ä¸å½±å“æ•´ä½“æµç¨‹
âœ… **å¯æ‰©å±•æ€§** - é¢„ç•™å¤šä¸ªæ‰©å±•ç‚¹ï¼Œæ”¯æŒè‡ªå®šä¹‰
âœ… **ç±»å‹å®‰å…¨** - ä½¿ç”¨ Pydantic è¿›è¡Œæ•°æ®éªŒè¯
âœ… **å¯è§‚æµ‹æ€§** - è¯¦ç»†çš„æ—¥å¿—å’Œç»Ÿè®¡ä¿¡æ¯

### å¾…æ”¹è¿›ç‚¹

ğŸš§ **å¢é‡åŒæ­¥** - å½“å‰ä¸ºå…¨é‡æ‰«æï¼Œéœ€è¦ä¼˜åŒ–
ğŸš§ **å¹¶å‘å¤„ç†** - æ–‡ä»¶å¤„ç†ä¸²è¡Œï¼Œå¯ä»¥å¹¶å‘ä¼˜åŒ–
ğŸš§ **æµ‹è¯•è¦†ç›–** - ç¼ºå°‘è‡ªåŠ¨åŒ–æµ‹è¯•
ğŸš§ **ç›‘æ§æŒ‡æ ‡** - ç¼ºå°‘ Prometheus æŒ‡æ ‡
ğŸš§ **é€Ÿç‡é™åˆ¶** - ç¼ºå°‘ API é€Ÿç‡é™åˆ¶
ğŸš§ **æ–‡ä»¶å¤§å°é™åˆ¶** - ç¼ºå°‘å¤§æ–‡ä»¶ä¿æŠ¤

### æŠ€æœ¯æ ˆ

- **è¯­è¨€**: Python 3.13+
- **æ¡†æ¶**: FastAPI + SQLModel
- **è§£æ**: python-frontmatter
- **æ•°æ®åº“**: PostgreSQL
- **å¼‚æ­¥**: asyncio
- **Git**: GitPython / subprocess

---

**æœ€åæ›´æ–°**: 2026-01-11
**æ–‡æ¡£ç‰ˆæœ¬**: 2.0.0
**ç»´æŠ¤è€…**: Blog Platform Team
