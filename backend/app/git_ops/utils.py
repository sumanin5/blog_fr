import logging
from pathlib import Path
from typing import Optional
from uuid import UUID

import httpx
from app.git_ops.exceptions import GitOpsSyncError, WebhookSignatureError

logger = logging.getLogger(__name__)


def verify_github_signature(payload: bytes, signature: str, secret: str) -> bool:
    """
    éªŒè¯ GitHub Webhook ç­¾åã€‚

    Args:
        payload: è¯·æ±‚ä½“ï¼ˆåŸå§‹å­—èŠ‚ï¼‰
        signature: GitHub å‘æ¥çš„ç­¾åï¼ˆæ ¼å¼ï¼šsha256=xxxï¼‰
        secret: Webhook secretï¼ˆä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰

    Returns:
        True å¦‚æœç­¾åæœ‰æ•ˆ

    Raises:
        WebhookSignatureError: å¦‚æœç­¾åæ— æ•ˆæˆ–ç¼ºå¤±
    """
    import hashlib
    import hmac

    if not secret:
        logger.warning(
            "âš ï¸ WEBHOOK_SECRET not configured. "
            "All webhook requests will be rejected for security."
        )
        raise WebhookSignatureError("Webhook secret not configured")

    if not signature:
        logger.warning("Missing X-Hub-Signature-256 header")
        raise WebhookSignatureError("Missing X-Hub-Signature-256 header")

    # ç”¨ secret å’Œ payload ç”Ÿæˆé¢„æœŸçš„ç­¾å
    expected = hmac.new(secret.encode(), payload, hashlib.sha256).hexdigest()
    expected_signature = f"sha256={expected}"

    # ä½¿ç”¨ compare_digest é˜²æ­¢æ—¶åºæ”»å‡»
    is_valid = hmac.compare_digest(expected_signature, signature)

    if not is_valid:
        logger.warning(
            f"Invalid webhook signature. Expected: {expected_signature[:20]}..., Got: {signature[:20]}..."
        )
        raise WebhookSignatureError("Invalid webhook signature")

    return True


async def update_frontmatter_metadata(
    content_dir, file_path: str, metadata: dict, stats
):
    """å°†å…ƒæ•°æ®å†™å›åˆ° MDX æ–‡ä»¶çš„ frontmatter

    æ”¯æŒæ›´æ–°å¤šä¸ªå­—æ®µï¼šslugã€author_idã€cover_media_idã€category_id ç­‰

    Args:
        content_dir: å†…å®¹ç›®å½•è·¯å¾„
        file_path: ç›¸å¯¹äº content_dir çš„æ–‡ä»¶è·¯å¾„
        metadata: è¦æ›´æ–°çš„å…ƒæ•°æ®å­—å…¸ {key: value, ...}
        stats: åŒæ­¥ç»Ÿè®¡å¯¹è±¡ï¼ˆç”¨äºè®°å½•é”™è¯¯ï¼‰

    Returns:
        True å¦‚æœæˆåŠŸï¼ŒFalse å¦‚æœå¤±è´¥
    """

    import frontmatter
    from app.git_ops.error_handler import handle_sync_error

    full_path = content_dir / file_path

    try:
        # è¯»å–æ–‡ä»¶
        with open(full_path, "r", encoding="utf-8") as f:
            post = frontmatter.load(f)

        # æ›´æ–°æ‰€æœ‰å…ƒæ•°æ®
        for key, value in metadata.items():
            if value is not None:
                post.metadata[key] = str(value)
            else:
                # å¦‚æœå€¼ä¸º Noneï¼Œåˆ é™¤è¯¥å­—æ®µ
                post.metadata.pop(key, None)

        # å†™å›æ–‡ä»¶
        with open(full_path, "w", encoding="utf-8") as f:
            f.write(frontmatter.dumps(post))

        logger.info(f"Updated frontmatter metadata: {file_path} -> {metadata}")
        return True
    except Exception as e:
        # å†™å›å¤±è´¥ä¸åº”è¯¥ä¸­æ–­åŒæ­¥æµç¨‹ï¼Œåªè®°å½•è­¦å‘Š
        handle_sync_error(
            stats,
            file_path=file_path,
            error_msg=f"Failed to update frontmatter: {str(e)}",
            is_critical=False,
        )
        return False


async def revalidate_nextjs_cache(frontend_url: str, revalidate_secret: str):
    """å¤±æ•ˆ Next.js ç¼“å­˜

    åœ¨ Git åŒæ­¥å®Œæˆåè°ƒç”¨ï¼Œé€šçŸ¥ Next.js å¤±æ•ˆç¼“å­˜ï¼Œ
    ç¡®ä¿ç”¨æˆ·ç«‹å³çœ‹åˆ°æœ€æ–°çš„æ–‡ç« å†…å®¹ã€‚

    Args:
        frontend_url: Next.js å‰ç«¯ URL
        revalidate_secret: ç¼“å­˜å¤±æ•ˆå¯†é’¥

    Returns:
        True å¦‚æœæˆåŠŸï¼ŒFalse å¦‚æœå¤±è´¥

    Raises:
        æ— å¼‚å¸¸ï¼Œå¤±è´¥æ—¶åªè®°å½•è­¦å‘Š
    """
    if not frontend_url or not revalidate_secret:
        logger.warning(
            "âš ï¸ FRONTEND_URL or REVALIDATE_SECRET not configured, "
            "skipping Next.js cache revalidation"
        )
        return False

    try:
        # è°ƒç”¨ Next.js API å¤±æ•ˆç¼“å­˜
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{frontend_url}/api/revalidate",
                headers={
                    "Authorization": f"Bearer {revalidate_secret}",
                    "Content-Type": "application/json",
                },
                json={
                    "tags": ["posts", "posts-list", "categories"],
                    "paths": ["/posts"],
                },
                timeout=10.0,
            )

            if response.status_code == 200:
                data = response.json()
                logger.info(f"âœ… Next.js cache revalidated successfully: {data}")
                return True
            else:
                logger.warning(
                    f"âŒ Failed to revalidate Next.js cache: "
                    f"{response.status_code} {response.text}"
                )
                return False
    except Exception as e:
        logger.warning(f"âŒ Error revalidating Next.js cache: {e}")
        return False


async def write_post_ids_to_frontmatter(
    content_dir, file_path: str, post, old_post, stats
):
    """å°†æ–‡ç« çš„ ID å†™å›åˆ° frontmatter

    ç”¨äº"å›ç­¾è®¡åˆ’"ï¼šåœ¨åˆ›å»ºæˆ–æ›´æ–°æ–‡ç« åï¼Œå°†ç”Ÿæˆçš„ ID å†™å›åˆ° MDX æ–‡ä»¶ï¼Œ
    è¿™æ ·ä¸‹æ¬¡åŒæ­¥æ—¶å¯ä»¥ç›´æ¥ç”¨ ID æŸ¥è¯¢ï¼Œæ— éœ€å¤æ‚çš„åå­—/è·¯å¾„åŒ¹é…ã€‚

    Args:
        content_dir: å†…å®¹ç›®å½•è·¯å¾„
        file_path: ç›¸å¯¹äº content_dir çš„æ–‡ä»¶è·¯å¾„
        post: æ–°åˆ›å»ºæˆ–æ›´æ–°åçš„ Post å¯¹è±¡
        old_post: æ—§çš„ Post å¯¹è±¡ï¼ˆå¦‚æœæ˜¯æ›´æ–°ï¼‰ï¼Œç”¨äºæ£€æµ‹å˜åŒ–ï¼›å¦‚æœæ˜¯åˆ›å»ºåˆ™ä¸º None
        stats: åŒæ­¥ç»Ÿè®¡å¯¹è±¡ï¼ˆç”¨äºè®°å½•é”™è¯¯ï¼‰

    Returns:
        True å¦‚æœæˆåŠŸï¼ŒFalse å¦‚æœå¤±è´¥
    """
    # ç¡®å®šéœ€è¦æ›´æ–°çš„å­—æ®µ
    metadata_to_update = {
        "slug": post.slug,
        "author_id": str(post.author_id),
        "cover_media_id": str(post.cover_media_id) if post.cover_media_id else None,
        "category_id": str(post.category_id) if post.category_id else None,
    }

    # å¦‚æœæ˜¯æ›´æ–°æ“ä½œï¼Œåªæ›´æ–°æœ‰å˜åŒ–çš„å­—æ®µ
    if old_post:
        metadata_to_update = {
            k: v
            for k, v in metadata_to_update.items()
            if (
                k == "slug"
                and v != old_post.slug
                or k == "author_id"
                and v != str(old_post.author_id)
                or k == "cover_media_id"
                and v
                != (str(old_post.cover_media_id) if old_post.cover_media_id else None)
                or k == "category_id"
                and v != (str(old_post.category_id) if old_post.category_id else None)
            )
        }

        # å¦‚æœæ²¡æœ‰å˜åŒ–ï¼Œç›´æ¥è¿”å›
        if not metadata_to_update:
            return True

    # å†™å›åˆ°æ–‡ä»¶
    return await update_frontmatter_metadata(
        content_dir, file_path, metadata_to_update, stats
    )


async def resolve_author_id(session, author_value: str) -> UUID:
    """æ ¹æ®ç”¨æˆ·åæŸ¥è¯¢ä½œè€… ID

    Args:
        session: æ•°æ®åº“ä¼šè¯
        author_value: ç”¨æˆ·åæˆ– UUID

    Returns:
        ç”¨æˆ· ID

    Raises:
        GitOpsSyncError: å¦‚æœä½œè€…ä¸å­˜åœ¨
    """
    from app.users import crud as user_crud

    if not author_value:
        raise GitOpsSyncError(
            "Author value is empty", detail="Author field cannot be empty"
        )

    # å°è¯•ä½œä¸º UUID è§£æ
    try:
        user_id = UUID(author_value)
        user = await user_crud.get_user_by_id(session, user_id)
        if user:
            return user.id
    except ValueError:
        pass

    # ä½œä¸ºç”¨æˆ·åæŸ¥è¯¢
    user = await user_crud.get_user_by_username(session, author_value)
    if user:
        logger.info(f"é€šè¿‡ç”¨æˆ·ååŒ¹é…åˆ°ä½œè€…: {author_value} -> {user.id}")
        return user.id

    # æœªæ‰¾åˆ°ç”¨æˆ·
    raise GitOpsSyncError(
        f"Author not found: {author_value}",
        detail=f"User '{author_value}' does not exist in database",
    )


async def resolve_cover_media_id(
    session, cover_value: str, mdx_file_path: str = None, content_dir: Path = None
) -> Optional[UUID]:
    """æ ¹æ®æ–‡ä»¶è·¯å¾„ã€æ–‡ä»¶åæˆ–å¤–éƒ¨ URL æŸ¥è¯¢/æ³¨å…¥åª’ä½“åº“ ID

    é€»è¾‘ï¼š
    1. å¦‚æœæ˜¯ UUID æ ¼å¼ï¼Œç›´æ¥å°è¯•æŸ¥è¯¢
    2. å¦‚æœæ˜¯ http(s):// å¼€å¤´ï¼Œè¯´æ˜æ˜¯å¤–éƒ¨ URLï¼ˆæš‚ä¸æ”¯æŒè‡ªåŠ¨ä¸‹è½½ï¼Œä»…è®°å½•ï¼‰
    3. å¦‚æœæ˜¯æœ¬åœ°ç›¸å¯¹è·¯å¾„ (å¦‚ ./img.png)ï¼Œå°è¯•è‡ªåŠ¨ä¸Šä¼ åˆ°åª’ä½“åº“
    4. å…œåº•ï¼šå°è¯•åœ¨åª’ä½“åº“ä¸­æœç´¢åŒåæ–‡ä»¶

    Args:
        session: æ•°æ®åº“ä¼šè¯
        cover_value: å°é¢è·¯å¾„å€¼ (Frontmatter ä¸­çš„å†…å®¹)
        mdx_file_path: å½“å‰ MDX æ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„ (ç”¨äºè§£ææœ¬åœ°ç›¸å¯¹è·¯å¾„)
        content_dir: Git å†…å®¹æ ¹ç›®å½•

    Returns:
        åª’ä½“æ–‡ä»¶ ID æˆ– None
    """
    from app.media import crud as media_crud
    from app.media import service as media_service
    from app.users import crud as user_crud

    if not cover_value:
        return None

    # 1. å°è¯•ä½œä¸º UUID è§£æ
    try:
        media_id = UUID(cover_value)
        media = await media_crud.get_media_file(session, media_id)
        if media:
            return media.id
    except ValueError:
        pass

    # 2. æ£€æŸ¥æ˜¯å¦æ˜¯å¤–éƒ¨ URL (TODO: å¢å¼º Media æ¨¡å‹ä»¥æ”¯æŒå¤–éƒ¨é“¾æ¥)
    if cover_value.startswith(("http://", "https://")):
        logger.warning(
            f"Detected external cover URL: {cover_value}. External URLs are not fully supported as Media entities yet."
        )
        # ç›®å‰ Media è¡¨ä¸»è¦å­˜å‚¨æ–‡ä»¶ï¼Œæš‚ä¸å¤„ç†å¼•ç”¨ï¼Œè¿”å› None ä»¥ä¿æŒå®‰å…¨
        return None

    # 3. å°è¯•æœ¬åœ°æ–‡ä»¶è‡ªåŠ¨ä¸Šä¼  (æ ¸å¿ƒ Git-First é€»è¾‘)
    # å¦‚æœæ˜¯ä»¥ ./ å¼€å¤´ï¼Œæˆ–è€… mdx_file_path å­˜åœ¨ä¸”å®ƒä¸åŒ…å« httpï¼Œæˆ‘ä»¬å°è¯•å®šä½ç‰©ç†æ–‡ä»¶
    if mdx_file_path and content_dir:
        # è®¡ç®—å›¾ç‰‡çš„ç»å¯¹è·¯å¾„
        mdx_dir = (content_dir / mdx_file_path).parent
        img_abs_path = (mdx_dir / cover_value).resolve()

        # ç¡®ä¿å›¾ç‰‡åœ¨ content_dir èŒƒå›´å†… (é˜²æ­¢è·¯å¾„ç©¿è¶Š)
        if (
            img_abs_path.exists()
            and img_abs_path.is_file()
            and str(img_abs_path).startswith(str(content_dir))
        ):
            # æ£€æŸ¥æ•°æ®åº“é‡Œæ˜¯å¦å·²ç»â€œä¸Šä¼ â€è¿‡è¿™ä¸ªåŸå§‹è·¯å¾„
            # æˆ‘ä»¬ç”¨åŸå§‹æ–‡ä»¶ååšä¸€æ¬¡ç®€å•åŒ¹é…ï¼Œæˆ–è€…æœªæ¥å¯ä»¥å¢åŠ ä¸€ä¸ªå­—æ®µå­˜å‚¨ git_source_path
            filename = img_abs_path.name
            media = await media_crud.get_media_file_by_path(
                session, filename
            )  # ç®€å•ç­–ç•¥ï¼šæŒ‰æ–‡ä»¶å

            if not media:
                logger.info(
                    f"ğŸš€ Found local cover image: {cover_value}, attempting auto-upload..."
                )
                try:
                    # è·å–ä¸€ä¸ªè¶…çº§ç®¡ç†å‘˜ä½œä¸ºä¸Šä¼ è€…
                    admin = await user_crud.get_superuser(session)
                    if not admin:
                        raise Exception("No superadmin found for auto-ingestion")

                    # è¯»å–å¹¶ä¸Šä¼ 
                    import asyncio

                    file_content = await asyncio.to_thread(img_abs_path.read_bytes)
                    media = await media_service.create_media_file(
                        file_content=file_content,
                        filename=filename,
                        uploader_id=admin.id,
                        session=session,
                        usage="post_cover",
                        is_public=True,
                        description=f"Auto-uploaded from git: {mdx_file_path}",
                    )
                    logger.info(f"âœ… Auto-uploaded cover: {filename} -> {media.id}")
                except Exception as e:
                    logger.error(f"âŒ Failed to auto-upload cover {cover_value}: {e}")
                    # å¤±è´¥äº†ä¸ä¸­æ–­æµç¨‹

            if media:
                return media.id

    # 4. å…œåº•ç­–ç•¥ï¼šå°è¯•ç²¾ç¡®è·¯å¾„åŒ¹é… (é’ˆå¯¹å·²å­˜åœ¨çš„ Media è®°å½•)
    media = await media_crud.get_media_file_by_path(session, cover_value)
    if media:
        logger.info(f"é€šè¿‡è·¯å¾„åŒ¹é…åˆ°å°é¢: {cover_value}")
        return media.id

    # 5. å°è¯•æ–‡ä»¶åæ¨¡ç³Šæœç´¢ç¼“å­˜
    filename = Path(cover_value).name
    results = await media_service.search_media_files(session, query=filename, limit=1)

    if results:
        logger.info(f"é€šè¿‡æ–‡ä»¶åæ¨¡ç³ŠåŒ¹é…åˆ°å°é¢: {filename} -> {results[0].file_path}")
        return results[0].id

    logger.warning(f"æœªæ‰¾åˆ°å°é¢å›¾: {cover_value}")
    return None


async def resolve_category_id(
    session,
    category_value: Optional[str],
    post_type: str,
    auto_create: bool = True,
    default_slug: str = "uncategorized",
) -> Optional[UUID]:
    """æ ¹æ® slug æŸ¥è¯¢æˆ–åˆ›å»ºåˆ†ç±»

    Args:
        session: æ•°æ®åº“ä¼šè¯
        category_value: åˆ†ç±» Slug
        post_type: æ–‡ç« ç±»å‹
        auto_create: æ˜¯å¦è‡ªåŠ¨åˆ›å»º
        default_slug: é»˜è®¤åˆ†ç±» Slug

    Returns:
        åˆ†ç±» ID æˆ– None
    """
    from app.posts import crud as posts_crud
    from app.posts.model import Category

    if not category_value:
        category_value = default_slug

    if hasattr(post_type, "value"):
        post_type = post_type.value

    # 1. å°è¯•æŸ¥è¯¢ç°æœ‰åˆ†ç±»
    category = await posts_crud.get_category_by_slug_and_type(
        session, category_value, post_type
    )

    if category:
        logger.info(f"é€šè¿‡ slug åŒ¹é…åˆ°åˆ†ç±»: {category_value}")
        return category.id

    # 2. å¦‚æœä¸å­˜åœ¨ä¸”å…è®¸è‡ªåŠ¨åˆ›å»º
    if auto_create and category_value != default_slug:
        logger.info(f"Creating new category: {category_value} (type={post_type})")
        name = category_value.replace("-", " ").title()
        new_category = Category(
            name=name,
            slug=category_value,
            post_type=post_type,
            description=f"Auto generated from folder {category_value}",
        )
        session.add(new_category)
        await session.commit()
        await session.refresh(new_category)
        return new_category.id

    # 3. å°è¯•é»˜è®¤åˆ†ç±»
    if category_value != default_slug:
        return await resolve_category_id(
            session, default_slug, post_type, auto_create, default_slug
        )

    # 4. é»˜è®¤åˆ†ç±»ä¹Ÿä¸å­˜åœ¨ï¼Œå°è¯•åˆ›å»º
    logger.warning(
        f"Default category '{default_slug}' not found for type '{post_type}'. Creating it."
    )
    default_cat = Category(
        name=default_slug.title(),
        slug=default_slug,
        post_type=post_type,
        description="Default Category",
    )
    session.add(default_cat)
    await session.commit()
    await session.refresh(default_cat)
    return default_cat.id


async def resolve_tag_ids(session, tag_names: list) -> list[UUID]:
    """æ ¹æ®æ ‡ç­¾åç§°æŸ¥è¯¢æˆ–åˆ›å»ºæ ‡ç­¾ï¼Œè¿”å›æ ‡ç­¾ ID åˆ—è¡¨

    Args:
        session: æ•°æ®åº“ä¼šè¯
        tag_names: æ ‡ç­¾åç§°åˆ—è¡¨

    Returns:
        æ ‡ç­¾ ID åˆ—è¡¨
    """
    from app.posts import crud as posts_crud
    from slugify import slugify as python_slugify

    if not tag_names:
        return []

    tag_ids = []

    for tag_name in tag_names:
        tag_name = tag_name.strip()
        if not tag_name:
            continue

        # ä½¿ç”¨ get_or_create_tag è·å–æˆ–åˆ›å»ºæ ‡ç­¾
        tag_slug = python_slugify(tag_name)
        tag = await posts_crud.get_or_create_tag(session, tag_name, tag_slug)
        logger.info(f"æ ‡ç­¾å·²å¤„ç†: {tag_name} -> {tag.id}")
        tag_ids.append(tag.id)

    return tag_ids


async def handle_post_update(
    session,
    matched_post,
    scanned,
    file_path: str,
    is_move: bool,
    mapper,
    operating_user,
    content_dir,
    stats,
    processed_post_ids: set,
    force_write: bool = False,
):
    """å¤„ç†æ–‡ç« æ›´æ–°æˆ–ç§»åŠ¨"""
    from app.posts import service as post_service
    from app.posts.schema import PostUpdate

    update_dict = await mapper.map_to_post(scanned)
    update_dict.pop("slug", None)
    update_dict.pop("tag_ids", None)

    if is_move:
        update_dict["source_path"] = file_path

    post_in = PostUpdate(**update_dict)
    updated_post = await post_service.update_post(
        session, matched_post.id, post_in, current_user=operating_user
    )
    await session.refresh(updated_post)

    # å¦‚æœ force_write ä¸º Trueï¼Œåˆ™ä¼ å…¥ old_post=Noneï¼Œå¼ºåˆ¶å†™å…¥æ‰€æœ‰å­—æ®µ
    old_post_arg = None if force_write else matched_post
    await write_post_ids_to_frontmatter(
        content_dir, file_path, updated_post, old_post_arg, stats
    )

    processed_post_ids.add(matched_post.id)
    stats.updated.append(file_path)

    return updated_post


async def handle_post_create(
    session,
    scanned,
    file_path: str,
    mapper,
    operating_user,
    content_dir,
    stats,
    processed_post_ids: set,
):
    """å¤„ç†æ–‡ç« åˆ›å»º"""
    from app.posts import service as post_service
    from app.posts.schema import PostCreate
    from app.posts.utils import generate_slug_with_random_suffix

    create_dict = await mapper.map_to_post(scanned)
    create_dict["source_path"] = file_path

    if not create_dict.get("slug"):
        create_dict["slug"] = generate_slug_with_random_suffix(Path(file_path).stem)

    create_dict.pop("tag_ids", None)

    post_in = PostCreate(**create_dict)
    created_post = await post_service.create_post(
        session, post_in, author_id=create_dict["author_id"]
    )

    await write_post_ids_to_frontmatter(
        content_dir, file_path, created_post, None, stats
    )

    processed_post_ids.add(created_post.id)
    stats.added.append(file_path)

    return created_post


async def validate_post_for_resync(session, content_dir, post_id):
    """éªŒè¯ Post æ˜¯å¦å¯ä»¥ resync

    Args:
        session: æ•°æ®åº“ä¼šè¯
        content_dir: å†…å®¹ç›®å½•
        post_id: æ–‡ç«  ID

    Returns:
        Post å¯¹è±¡

    Raises:
        GitOpsSyncError: å¦‚æœéªŒè¯å¤±è´¥
    """
    from app.posts import crud as posts_crud

    post = await posts_crud.get_post_by_id(session, post_id)
    if not post:
        raise GitOpsSyncError(
            f"Post not found: {post_id}",
            detail="Cannot resync metadata for non-existent post",
        )

    if not post.source_path:
        raise GitOpsSyncError(
            f"Post {post_id} has no source_path",
            detail="Only posts synced from Git can resync metadata",
        )

    file_path = content_dir / post.source_path
    if not file_path.exists():
        raise GitOpsSyncError(
            f"Source file not found: {post.source_path}",
            detail="The MDX file may have been deleted or moved",
        )

    return post
