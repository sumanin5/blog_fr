import asyncio
import logging
from pathlib import Path
from typing import List, Optional

import frontmatter
import orjson
from app.git_ops.exceptions import GitOpsConfigurationError, ScanError

from .models import ScannedPost
from .path_parser import PathParser
from .utils import calc_hash

logger = logging.getLogger(__name__)


class MDXScanner:
    def __init__(self, content_root: Path, path_parser: Optional[PathParser] = None):
        self.content_root = Path(content_root)
        self.path_parser = path_parser or PathParser()

        if not self.content_root.exists():
            raise GitOpsConfigurationError(
                f"Content root does not exist: {content_root}"
            )

    async def scan_file(self, rel_path: str) -> Optional[ScannedPost]:
        """è§£æå•ä¸ªæ–‡ä»¶ã€‚å¼‚å¸¸å°†è¢«ç»Ÿä¸€åŒ…è£…ä¸º ScanError ä»¥ä¾¿å…¨å±€å¤„ç†ã€‚"""

        full_path = self.content_root / rel_path
        if not full_path.is_file():
            return None

        try:
            # 1. å¼‚æ­¥è¯»å–ä¸è§£æ
            raw_content = await asyncio.to_thread(full_path.read_text, encoding="utf-8")
            post = frontmatter.loads(raw_content)  # ä¸»è¦çš„æ€§èƒ½ç“¶é¢ˆ

            # 2. è®¡ç®— Hash ä¸è·¯å¾„è§£æ
            # ä½¿ç”¨ orjson è¿›è¡Œæ›´å¿«çš„ã€ç¡®å®šæ€§çš„åºåˆ—åŒ– (è‡ªåŠ¨å¤„ç†æ—¥æœŸç­‰)
            meta_bytes = orjson.dumps(post.metadata, option=orjson.OPT_SORT_KEYS)
            path_info = self.path_parser.parse(rel_path)

            # æ£€æµ‹æ˜¯å¦ä¸ºåˆ†ç±»å…ƒæ•°æ®æ–‡ä»¶
            is_index = Path(rel_path).name.lower() == "index.md"
            is_category_index = is_index and bool(path_info.get("category_slug"))

            return ScannedPost(
                file_path=str(rel_path),
                content_hash=calc_hash(raw_content),
                meta_hash=calc_hash(meta_bytes),
                frontmatter=post.metadata,
                content=post.content,
                updated_at=full_path.stat().st_mtime,
                derived_post_type=path_info.get("post_type"),
                derived_category_slug=path_info.get("category_slug"),
                is_category_index=is_category_index,
            )
        except Exception as e:
            # é™„å¸¦æ–‡ä»¶è·¯å¾„ä¸Šä¸‹æ–‡ï¼Œç¬¦åˆå…¨å±€é”™è¯¯å¤„ç†è§„èŒƒ
            raise ScanError(rel_path, str(e)) from e

    async def scan_all(
        self, glob_patterns: Optional[List[str]] = None
    ) -> List[ScannedPost]:
        """
        æ‰«ææ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶ (å¹¶å‘æ¨¡å¼ï¼Œå¸¦é™æµä¿æŠ¤)ã€‚

        ä¼˜åŒ–ç‚¹ï¼š
        1. è·¯å¾„å»é‡ï¼šä½¿ç”¨ set é¿å…å¤šä¸ª pattern é‡å¤åŒ¹é…åŒä¸€æ–‡ä»¶ã€‚
        2. å¹¶å‘é™æµï¼šä½¿ç”¨ Semaphore é˜²æ­¢ç¬é—´æ‰“å¼€è¿‡å¤šæ–‡ä»¶å¥æŸ„ã€‚
        3. å®¹é”™å¢å¼ºï¼šä¿æŒ gather çš„ return_exceptions=Trueï¼Œç¡®ä¿ä¸ªåˆ«æ–‡ä»¶æŸåä¸å½±å“å…¨å±€ã€‚
        """
        if glob_patterns is None:
            target_extensions = {".md", ".mdx"}
        else:
            # ä» pattern ä¸­æå–åç¼€åè¿›è¡Œä¼˜åŒ–è¿‡æ»¤ï¼Œå¯ä»¥è‡ªå®šä¹‰
            target_extensions = {
                f".{p.split('.')[-1].lower()}" for p in glob_patterns if "." in p
            }

        # 1. å‘ç°å¹¶æ”¶é›†è·¯å¾„ (å•æ¬¡é€’å½’éå†ç‰©ç†ç£ç›˜ï¼Œæ•ˆç‡æ›´é«˜)
        target_path_set = set()
        ignore_names = {"README.MD", "README.MDX", "LICENSE.MD", ".GITIGNORE"}

        # ä»…æ‰§è¡Œä¸€æ¬¡ rglobï¼Œåœ¨å¾ªç¯å†…è¿›è¡Œåç¼€åŒ¹é…
        for path in self.content_root.rglob("*"):
            # è·å–ç›¸å¯¹äºæ ¹ç›®å½•çš„å†…å®¹ï¼Œæ£€æŸ¥æ¯ä¸€çº§æ˜¯å¦åŒ…å«éšè—ç›®å½•/æ–‡ä»¶
            rel_path_obj = path.relative_to(self.content_root)

            if (
                # å››é‡è¿‡æ»¤æ¡ä»¶ï¼šæ–‡ä»¶ã€åç¼€ã€åç§°ã€ééšè—è·¯å¾„
                path.is_file()
                and path.suffix.lower() in target_extensions
                and path.name.upper() not in ignore_names
                and not any(part.startswith(".") for part in rel_path_obj.parts)
            ):
                target_path_set.add(path)

        if not target_path_set:
            return []

        # è½¬æ¢ä¸ºç›¸å¯¹è·¯å¾„
        rel_paths = [p.relative_to(self.content_root) for p in target_path_set]
        logger.info(f"ğŸ” [Scanner] Found {len(rel_paths)} target files to scan.")

        # 2. å¹¶å‘æ‰«æ (å¼•å…¥ä¿¡å·é‡é™æµï¼Œé»˜è®¤å¹¶å‘æ•°ä¸º 20)
        # è¿™èƒ½ä¿è¯å³ä¾¿æ–‡ä»¶æå¤šï¼Œä¹Ÿä¸ä¼šå› ç¬é—´å†…å­˜å³°å€¼æˆ–æ–‡ä»¶å¥æŸ„è€—å°½è€Œå´©æºƒ
        sem = asyncio.Semaphore(20)

        async def throttled_scan(rel_p: Path):
            async with sem:
                return await self.scan_file(str(rel_p))

        tasks = [throttled_scan(p) for p in rel_paths]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # 3. ç­›é€‰å¹¶æ±‡æ€»ç»“æœ
        scanned_posts = []
        for i, res in enumerate(results):
            if isinstance(res, Exception):
                logger.error(f"Failed to scan file {rel_paths[i]}: {res}")
            elif res:
                scanned_posts.append(res)

        return scanned_posts
