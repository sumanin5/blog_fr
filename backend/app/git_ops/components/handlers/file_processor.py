"""
æ–‡ä»¶å¤„ç†å™¨ - ç»Ÿä¸€å¤„ç†æ–‡ä»¶çš„æ–°å¢ã€æ›´æ–°ã€åˆ é™¤é€»è¾‘
"""

import logging
from pathlib import Path
from typing import Any, Dict

from sqlmodel.ext.asyncio.session import AsyncSession

from app.git_ops.components.handlers.category_sync import handle_category_sync
from app.git_ops.components.handlers.post_create import handle_post_create
from app.git_ops.components.handlers.post_update import handle_post_update
from app.git_ops.components.scanner import MDXScanner
from app.git_ops.components.serializer import PostSerializer
from app.git_ops.schema import SyncStats
from app.posts import services as post_service
from app.posts.model import Post
from app.users.model import User

logger = logging.getLogger(__name__)


class SyncProcessor:
    """åŒæ­¥å¤„ç†å™¨ - è´Ÿè´£å…·ä½“çš„åŒæ­¥é€»è¾‘"""

    def __init__(
        self,
        scanner: MDXScanner,
        serializer: PostSerializer,
        content_dir: Path,
    ):
        self.scanner = scanner
        self.serializer = serializer
        self.content_dir = content_dir

    async def process_file_change(
        self,
        session: AsyncSession,
        file_path: str,
        status: str,  # "A" (added), "M" (modified), "D" (deleted)
        existing_map: Dict[str, Post],
        operating_user: User,
        stats: SyncStats,
        processed_post_ids: set,
    ):
        """
        ç»Ÿä¸€å¤„ç†æ–‡ä»¶å˜æ›´ï¼ˆæ–°å¢ã€ä¿®æ”¹ã€åˆ é™¤ï¼‰

        Args:
            session: æ•°æ®åº“ä¼šè¯
            file_path: æ–‡ä»¶è·¯å¾„
            status: å˜æ›´çŠ¶æ€ ("A", "M", "D")
            existing_map: ç°æœ‰æ–‡ç« æ˜ å°„ {source_path: Post}
            operating_user: æ“ä½œç”¨æˆ·
            stats: ç»Ÿè®¡ä¿¡æ¯
            processed_post_ids: å·²å¤„ç†çš„æ–‡ç«  ID é›†åˆ
        """

        # åˆ é™¤æ–‡ä»¶
        if status == "D":
            post = existing_map.get(file_path)
            if post:
                logger.info(f"ğŸ—‘ï¸  Deleting post: {file_path}")
                await post_service.delete_post(
                    session, post.id, current_user=operating_user
                )
                stats.deleted.append(str(file_path))
            else:
                logger.warning(
                    f"âš ï¸  File marked as deleted but not found in DB: {file_path}"
                )
            return

        # æ–°å¢æˆ–ä¿®æ”¹æ–‡ä»¶
        if status in ("A", "M"):
            # æ‰«ææ–‡ä»¶
            scanned = await self.scanner.scan_file(file_path)

            # å¤„ç†åˆ†ç±» index
            if scanned.is_category_index:
                logger.info(f"ğŸ”„ Processing category index: {scanned.file_path}")
                category = await handle_category_sync(
                    session,
                    scanned,
                    operating_user,
                    self.content_dir,
                )
                if category:
                    logger.info(
                        f"âœ… Category synced: {category.name} (slug={category.slug})"
                    )
                stats.updated.append(str(scanned.file_path))
                return

            # å¤„ç†æ–‡ç« ï¼šåˆ¤æ–­æ˜¯æ–°å¢è¿˜æ˜¯æ›´æ–°
            if file_path not in existing_map:
                # æ–°å¢æ–‡ç« 
                logger.info(f"â• Creating new post: {file_path}")
                await handle_post_create(
                    session,
                    scanned,
                    file_path,
                    self.serializer,
                    operating_user,
                    self.content_dir,
                    stats,
                    processed_post_ids,
                )
            else:
                # æ›´æ–°å·²æœ‰æ–‡ç« 
                post = existing_map[file_path]
                logger.info(f"ğŸ“ Updating existing post: {file_path}")
                await handle_post_update(
                    session,
                    post,
                    scanned,
                    Path(file_path) if isinstance(file_path, str) else file_path,
                    False,  # is_move
                    self.serializer,
                    operating_user,
                    self.content_dir,
                    stats,
                    processed_post_ids,
                )
            return

        # æœªçŸ¥çŠ¶æ€
        logger.warning(f"âš ï¸  Unknown file status '{status}' for: {file_path}")

    async def process_scanned_file(
        self,
        session: AsyncSession,
        file_path: str,
        scanned,
        existing_map: Dict[str, Post],
        operating_user: User,
        stats: SyncStats,
        processed_post_ids: set,
    ):
        """
        å¤„ç†å·²æ‰«æçš„æ–‡ä»¶ï¼ˆç”¨äºå…¨é‡åŒæ­¥ï¼‰

        Args:
            session: æ•°æ®åº“ä¼šè¯
            file_path: æ–‡ä»¶è·¯å¾„
            scanned: æ‰«æç»“æœ
            existing_map: ç°æœ‰æ–‡ç« æ˜ å°„ {source_path: Post}
            operating_user: æ“ä½œç”¨æˆ·
            stats: ç»Ÿè®¡ä¿¡æ¯
            processed_post_ids: å·²å¤„ç†çš„æ–‡ç«  ID é›†åˆ
        """

        # å¤„ç†åˆ†ç±» index
        if scanned.is_category_index:
            logger.info(f"ğŸ”„ Processing category index: {scanned.file_path}")
            category = await handle_category_sync(
                session,
                scanned,
                operating_user,
                self.content_dir,
            )
            if category:
                logger.info(
                    f"âœ… Category synced: {category.name} (slug={category.slug})"
                )
            stats.updated.append(str(scanned.file_path))
            return

        # å¤„ç†æ–‡ç« ï¼šæ˜¾å¼åˆ¤æ–­æ˜¯æ–°å¢è¿˜æ˜¯æ›´æ–°
        if file_path not in existing_map:
            # æ–°å¢æ–‡ç« 
            logger.info(f"â• Creating new post: {file_path}")
            await handle_post_create(
                session,
                scanned,
                file_path,
                self.serializer,
                operating_user,
                self.content_dir,
                stats,
                processed_post_ids,
            )
        else:
            # æ›´æ–°å·²æœ‰æ–‡ç« 
            post = existing_map[file_path]
            logger.info(f"ğŸ“ Updating existing post: {file_path}")
            await handle_post_update(
                session,
                post,
                scanned,
                Path(file_path) if isinstance(file_path, str) else file_path,
                False,  # is_move
                self.serializer,
                operating_user,
                self.content_dir,
                stats,
                processed_post_ids,
            )
            await handle_post_update(
                session,
                post,
                scanned,
                Path(file_path) if isinstance(file_path, str) else file_path,
                False,  # is_move
                self.serializer,
                operating_user,
                self.content_dir,
                stats,
                processed_post_ids,
            )

    async def reconcile_full_sync(
        self,
        session: AsyncSession,
        scanned_map: Dict[str, Any],
        existing_map: Dict[str, Post],
        operating_user: User,
        stats: SyncStats,
    ):
        """
        å…¨é‡åŒæ­¥çš„æ ¸å¿ƒåè°ƒé€»è¾‘ï¼š
        1. å¯¹æ¯”æ‰¾å‡ºå­¤å„¿è®°å½•å¹¶åˆ é™¤ (DB - Disk)
        2. éå†å¤„ç†æ‰€æœ‰ç£ç›˜æ–‡ä»¶ (Disk -> DB)

        Args:
            session: æ•°æ®åº“ä¼šè¯
            scanned_map: ç£ç›˜æ–‡ä»¶æ˜ å°„ {path: ScannedPost}
            existing_map: æ•°æ®åº“ç°æœ‰è®°å½•æ˜ å°„ {path: Post}
            operating_user: æ“ä½œç”¨æˆ·
            stats: ç»Ÿè®¡å¯¹è±¡
        """
        from app.git_ops.exceptions import collect_errors

        # 1. åˆ é™¤æ•°æ®åº“ä¸­å¤šä½™çš„è®°å½•ï¼ˆæ–‡ä»¶ç³»ç»Ÿä¸­ä¸å­˜åœ¨çš„ï¼‰
        for db_path, post in existing_map.items():
            if db_path not in scanned_map:
                async with collect_errors(stats, f"Deleting orphaned {db_path}"):
                    logger.info(f"Deleting orphaned post: {db_path} (slug={post.slug})")
                    await post_service.delete_post(
                        session, post.id, current_user=operating_user
                    )
                    stats.deleted.append(str(db_path))

        # 2. å¤„ç†æ‰«æåˆ°çš„æ–‡ä»¶ (Disk -> DB)
        processed_post_ids = set()
        for file_path, scanned in scanned_map.items():
            async with collect_errors(stats, f"Processing {file_path}"):
                await self.process_scanned_file(
                    session,
                    file_path,
                    scanned,
                    existing_map,
                    operating_user,
                    stats,
                    processed_post_ids,
                )

    async def sync_categories_to_disk(
        self, session: AsyncSession, writer, stats: SyncStats
    ):
        """
        (Disk -> DB ä¼˜å…ˆ) åªä¸ºå·²å­˜åœ¨çš„åˆ†ç±»ç›®å½•åˆ›å»º/æ›´æ–° index.md
        å¹¶åˆ é™¤ Git ä¸­å·²åˆ é™¤çš„åˆ†ç±»

        Git ä»“åº“æ˜¯çœŸç†æºï¼ˆSource of Truthï¼‰ï¼š
        - åªæœ‰å½“åˆ†ç±»ç›®å½•åœ¨æ–‡ä»¶ç³»ç»Ÿä¸­å­˜åœ¨æ—¶ï¼Œæ‰ä¼šåˆ›å»º/æ›´æ–° index.md
        - å¦‚æœç›®å½•ä¸å­˜åœ¨ï¼Œè¯´æ˜å·²åœ¨ Git ä¸­åˆ é™¤ï¼Œä»æ•°æ®åº“ä¸­åˆ é™¤è¯¥åˆ†ç±»
        - è¿™ç¡®ä¿äº† Git ä»“åº“çš„å˜æ›´ä¼˜å…ˆçº§é«˜äºæ•°æ®åº“

        Args:
            session: æ•°æ®åº“ä¼šè¯
            writer:FileWriter å®ä¾‹
            stats: ç»Ÿè®¡å¯¹è±¡
        """
        import frontmatter

        from app.posts.cruds import category as category_crud

        categories = await category_crud.get_all_categories(session)
        categories_to_delete = []

        for category in categories:
            try:
                target_path = writer.path_calculator.calculate_category_path(category)
                category_dir = target_path.parent

                # å…³é”®æ”¹å˜ï¼šå¦‚æœåˆ†ç±»ç›®å½•ä¸å­˜åœ¨ï¼Œæ ‡è®°ä¸ºåˆ é™¤
                if not category_dir.exists():
                    logger.info(
                        f"Category directory '{category.slug}' not found in Git, "
                        f"marking for deletion from database"
                    )
                    categories_to_delete.append(category)
                    continue

                # æ„å»ºæœŸæœ›çš„å†…å®¹
                meta = {"title": category.name, "hidden": not category.is_active}
                if category.icon_preset:
                    meta["icon"] = category.icon_preset
                if category.sort_order != 0:
                    meta["order"] = category.sort_order
                if category.excerpt:
                    meta["excerpt"] = category.excerpt
                if category.cover_media_id:
                    meta["cover_media_id"] = str(category.cover_media_id)
                    if hasattr(category, "cover_media") and category.cover_media:
                        meta["cover"] = category.cover_media.original_filename

                expected_content = frontmatter.dumps(
                    frontmatter.Post(category.description or "", **meta)
                )

                should_write = False
                if not target_path.exists():
                    # ç›®å½•å­˜åœ¨ä½† index.md ä¸å­˜åœ¨ï¼Œåˆ›å»ºå®ƒ
                    should_write = True
                    logger.info(
                        f"Creating missing index.md for existing category directory: {category.slug}"
                    )
                else:
                    existing_content = await writer.file_operator.read_text(target_path)
                    if existing_content.strip() != expected_content.strip():
                        should_write = True
                        logger.debug(
                            f"Updating index.md for category '{category.slug}' due to metadata changes"
                        )

                if should_write:
                    is_new = not target_path.exists()
                    await writer.write_category(category)
                    rel_path = target_path.relative_to(self.content_dir)
                    if is_new:
                        if str(rel_path) not in stats.added:
                            stats.added.append(str(rel_path))
                    else:
                        if str(rel_path) not in stats.updated:
                            stats.updated.append(str(rel_path))
            except Exception as e:
                logger.error(f"Failed to sync category index for {category.slug}: {e}")

        # åˆ é™¤ Git ä¸­å·²ä¸å­˜åœ¨çš„åˆ†ç±»
        if categories_to_delete:
            for category in categories_to_delete:
                try:
                    logger.info(
                        f"Deleting category '{category.name}' (slug: {category.slug}) "
                        f"as its directory was removed from Git"
                    )
                    await session.delete(category)
                except Exception as e:
                    logger.error(
                        f"Failed to delete category '{category.slug}' from database: {e}"
                    )

    async def reconcile_incremental_sync(
        self,
        session: AsyncSession,
        changed_files: list,
        existing_map: Dict[str, Post],
        operating_user: User,
        stats: SyncStats,
    ):
        """
        å¢é‡åŒæ­¥çš„æ ¸å¿ƒåè°ƒé€»è¾‘ï¼š
        éå†å˜æ›´åˆ—è¡¨å¹¶è°ƒåº¦å¤„ç†

        Args:
            session: æ•°æ®åº“ä¼šè¯
            changed_files: å˜æ›´æ–‡ä»¶åˆ—è¡¨ [(status, path), ...]
            existing_map: æ¶‰åŠåˆ°çš„æ•°æ®åº“ç°æœ‰è®°å½•æ˜ å°„
            operating_user: æ“ä½œç”¨æˆ·
            stats: ç»Ÿè®¡å¯¹è±¡
        """
        from app.git_ops.exceptions import collect_errors

        processed_post_ids = set()

        if changed_files:
            logger.info(
                f"Incremental sync: processing {len(changed_files)} changed files."
            )
            for status, file_path in changed_files:
                if not file_path.endswith((".md", ".mdx")):
                    continue

                async with collect_errors(stats, f"Processing {status} {file_path}"):
                    await self.process_file_change(
                        session,
                        file_path,
                        status,
                        existing_map,
                        operating_user,
                        stats,
                        processed_post_ids,
                    )
